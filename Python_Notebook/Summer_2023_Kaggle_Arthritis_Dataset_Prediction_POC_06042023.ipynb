{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_5d_nM_9PLVt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fthH7oGJiOZJ",
        "outputId": "c87da8a0-288e-4cf6-d7dc-698ded4dece7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-06-04 15:17:07--  https://github.com/ShreyaJaiswal1604/Detecting_Arthritis_X-Ray_Images/archive/refs/heads/main.zip\n",
            "Connecting to 10.99.0.130:3128... connected.\n",
            "Proxy request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/ShreyaJaiswal1604/Detecting_Arthritis_X-Ray_Images/zip/refs/heads/main [following]\n",
            "--2023-06-04 15:17:07--  https://codeload.github.com/ShreyaJaiswal1604/Detecting_Arthritis_X-Ray_Images/zip/refs/heads/main\n",
            "Connecting to 10.99.0.130:3128... connected.\n",
            "Proxy request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘/tmp/Arthritis-dataset.zip’\n",
            "\n",
            "/tmp/Arthritis-data     [            <=>     ]  89.88M  38.1MB/s    in 2.4s    \n",
            "\n",
            "2023-06-04 15:17:10 (38.1 MB/s) - ‘/tmp/Arthritis-dataset.zip’ saved [94250618]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    \"https://github.com/ShreyaJaiswal1604/Detecting_Arthritis_X-Ray_Images/archive/refs/heads/main.zip\" \\\n",
        "    -O \"/tmp/Arthritis-dataset.zip\"\n",
        "\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile('/tmp/Arthritis-dataset.zip', 'r') #Opens the zip file in read mode\n",
        "zip_ref.extractall('/tmp') #Extracts the files into the /tmp folder\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BGE4hB4Citj"
      },
      "outputs": [],
      "source": [
        "# Define dataset paths\n",
        "base_dir = '/tmp/Detecting_Arthritis_X-Ray_Images-main/Image_Datasets/Original_Kaggle_Binary_Dataset'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "val_dir = os.path.join(base_dir, 'val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoFWyZ8xniGv",
        "outputId": "c4975bc0-2012-435c-9ebc-40d5e1c59566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_dir /tmp/Detecting_Arthritis_X-Ray_Images-main/Image_Datasets/Original_Kaggle_Binary_Dataset\n",
            "train_dir /tmp/Detecting_Arthritis_X-Ray_Images-main/Image_Datasets/Original_Kaggle_Binary_Dataset/train\n",
            "test_dir /tmp/Detecting_Arthritis_X-Ray_Images-main/Image_Datasets/Original_Kaggle_Binary_Dataset/test\n",
            "val_dir /tmp/Detecting_Arthritis_X-Ray_Images-main/Image_Datasets/Original_Kaggle_Binary_Dataset/val\n"
          ]
        }
      ],
      "source": [
        "print(\"base_dir\", base_dir)\n",
        "print(\"train_dir\", train_dir)\n",
        "print(\"test_dir\", test_dir)\n",
        "print(\"val_dir\", val_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5yNpVKkPfS2"
      },
      "outputs": [],
      "source": [
        "# Define image data generators with data augmentation for training and normalization for validation/testing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,           # Normalize pixel values to [0, 1]\n",
        "    rotation_range=20,        # Randomly rotate images\n",
        "    width_shift_range=0.1,    # Randomly shift images horizontally\n",
        "    height_shift_range=0.1,   # Randomly shift images vertically\n",
        "    shear_range=0.1,          # Apply shear transformation\n",
        "    zoom_range=0.1,           # Apply zoom transformation\n",
        "    horizontal_flip=True,     # Flip images horizontally\n",
        "    fill_mode='nearest'       # Fill newly created pixels after rotation or shifting\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlKUozjmYCCP"
      },
      "outputs": [],
      "source": [
        "test_val_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale pixel values to [0, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A7Maz80YETF"
      },
      "outputs": [],
      "source": [
        "# Define batch size and image size\n",
        "batch_size = 32\n",
        "image_size = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz9kmQ3gYGPC",
        "outputId": "e9ba96af-6cb5-4162-dbc0-e71e13bc7bd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2459 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Create data generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoT3XMSHnzS-",
        "outputId": "be8df795-eaf8-4056-e904-ed1293f62c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 690 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "test_generator = test_val_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBcnO_TRnz66",
        "outputId": "15c5367a-2e8c-4803-852a-a66041de1b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 355 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "val_generator = test_val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwkGd4TEn4mU",
        "outputId": "f045cae8-acbe-420e-d622-6b30fd100540"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-04 15:17:14.938731: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        }
      ],
      "source": [
        "# Define the model architecture\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.applications.DenseNet201(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_shape=(224, 224, 3)\n",
        "    ),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i52lJ1nT8oOy"
      },
      "source": [
        "---\n",
        "**CODE EXPLANATION**\n",
        "\n",
        "---\n",
        "The given code defines the architecture of a model using the Keras Sequential API. Here's an explanation of each component:\n",
        "\n",
        "tf.keras.applications.DenseNet201: This is a pre-trained DenseNet201 model provided by Keras. By setting include_top=False, we exclude the fully connected layers at the top of the network. This allows us to use the pre-trained convolutional base layers for feature extraction.\n",
        "\n",
        "**weights='imagenet':** This parameter specifies that we want to use the pre-trained weights of the DenseNet201 model that were trained on the ImageNet dataset. These weights capture general image features and can be useful for transfer learning.\n",
        "\n",
        "**input_shape=(224, 224, 3):** This defines the input shape of the model. In this case, the input shape is set to (224, 224, 3), indicating that the model expects input images of size 224x224 pixels with 3 color channels (RGB).\n",
        "\n",
        "**tf.keras.layers.GlobalAveragePooling2D():** This layer performs global average pooling on the output of the pre-trained DenseNet201 model. It reduces the spatial dimensions of the feature maps to a vector by taking the average value across each feature map. This helps to extract a compact representation of the features.\n",
        "\n",
        "**tf.keras.layers.Dense(1, activation='sigmoid'):** This is the final dense layer of the model with 1 neuron. It uses the sigmoid activation function, which maps the output to a range of [0, 1], representing the probability of the input belonging to the positive class. In this case, since it's a binary classification task (healthy or arthritis), we have a single output neuron.\n",
        "\n",
        "By combining these layers in a sequential manner, we create a model architecture that takes an input image, passes it through the pre-trained DenseNet201 base, performs global average pooling, and finally passes the result through a dense layer with sigmoid activation to obtain the output prediction.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XV76DZMWn8ux"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOLMvmEWoCst",
        "outputId": "eec08663-6e9c-4779-e155-d2de45d75498"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-04 15:17:42.445607: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77/77 [==============================] - ETA: 0s - loss: 0.1835 - accuracy: 0.9305 "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-04 16:06:23.808050: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "77/77 [==============================] - 3006s 38s/step - loss: 0.1835 - accuracy: 0.9305 - val_loss: 1.9900 - val_accuracy: 0.9127\n",
            "Epoch 2/5\n",
            "77/77 [==============================] - 2936s 38s/step - loss: 0.0636 - accuracy: 0.9797 - val_loss: 0.3113 - val_accuracy: 0.9239\n",
            "Epoch 4/5\n",
            "77/77 [==============================] - 2932s 38s/step - loss: 0.0553 - accuracy: 0.9833 - val_loss: 0.0582 - val_accuracy: 0.9718\n",
            "Epoch 5/5\n",
            "77/77 [==============================] - 2944s 38s/step - loss: 0.0417 - accuracy: 0.9870 - val_loss: 0.0930 - val_accuracy: 0.9718\n"
          ]
        }
      ],
      "source": [
        "# Train the model  \n",
        "print(\"START TIME : \") \n",
        "start_ts = time.time()\n",
        "print(\"----------------------------------------\")     \n",
        "                                          \n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=val_generator\n",
        ")\n",
        "print(\"END TIME : \") \n",
        "end_ts = time.time()\n",
        "print(\"----------------------------------------\")  \n",
        "# convert timestamps to datetime object\n",
        "dt1 = datetime.fromtimestamp(start_ts)\n",
        "print('Datetime Start:', dt1)\n",
        "dt2 = datetime.fromtimestamp(end_ts)\n",
        "print('Datetime End:', dt2)\n",
        "\n",
        "# Difference between two timestamps\n",
        "# in hours:minutes:seconds format\n",
        "delta = dt2 - dt1\n",
        "print(\"----------------------------------------\")  \n",
        "print('Difference is:', delta)\n",
        "print(\"----------------------------------------\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzTnx7odxKC-"
      },
      "source": [
        "---\n",
        "**EXPLANATION**\n",
        "\n",
        "---\n",
        "The given code is training the model using the fit method of the Keras Model class. Here's what each parameter does:\n",
        "\n",
        "**train_generator:** It is the generator that yields batches of training data, consisting of input images and their corresponding labels. It is used to train the model on the training dataset.\n",
        "\n",
        "**epochs:** It specifies the number of times the model will iterate over the entire training dataset during training. In this case, the model will be trained for 10 epochs.\n",
        "\n",
        "**validation_data:** It is the generator that yields batches of validation data, used to evaluate the model's performance on a separate dataset during training. The validation data is used to monitor the model's progress and prevent overfitting.\n",
        "\n",
        "During the training process, the model's weights and biases are updated iteratively to minimize the loss function defined in the model's compilation step. The fit method performs forward and backward propagation, calculates gradients, and updates the model parameters using an optimizer. The training process continues for the specified number of epochs, with the model gradually improving its performance on the training and validation data.\n",
        "\n",
        "\n",
        "---\n",
        "In the context of training a machine learning model, \"epochs\" refers to the number of times the model will iterate over the entire training dataset during the training process. Each epoch consists of going through the entire dataset once and updating the model's parameters based on the observed errors.\n",
        "\n",
        "When you set epochs=10 in the code, it means that the model will undergo 10 complete passes over the training dataset. In each epoch, the model will make predictions on the training data, calculate the loss (error), and adjust its internal parameters (weights and biases) based on the gradient of the loss function. By repeating this process for multiple epochs, the model learns from the data and improves its ability to make accurate predictions.\n",
        "\n",
        "The number of epochs is a hyperparameter that needs to be carefully chosen. **Too few epochs may result in underfitting, where the model fails to capture the underlying patterns in the data. On the other hand, too many epochs may lead to overfitting, where the model becomes overly specialized to the training data and performs poorly on unseen data. The optimal number of epochs can vary depending on the complexity of the problem and the size of the dataset, and it often requires experimentation and validation on a separate validation set.**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXdkbyxHoF5D",
        "outputId": "f41e9ee4-2565-419e-e2c1-493bedcf6b6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-04 19:25:29.003387: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - 130s 6s/step - loss: 0.1331 - accuracy: 0.9623\n",
            "Test Loss: 0.1331\n",
            "Test Accuracy: 0.9623\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UHUXAT6qs9v"
      },
      "outputs": [],
      "source": [
        "# Get the true labels for the test set\n",
        "y_true = test_generator.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1_KfLR7qs24",
        "outputId": "3d115641-42c2-4d1c-e540-012de7024a7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-06-04 19:50:43.032243: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
            "\t [[{{node Placeholder/_0}}]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22/22 [==============================] - 130s 6s/step\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the test set\n",
        "y_pred_prob = model.predict(test_generator)\n",
        "y_pred = np.round(y_pred_prob).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suJ63F6zCito",
        "outputId": "808b1549-f9cb-496a-8d89-4dd7efd2bbca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1.\n",
            " 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksSyZv8WrDRq",
        "outputId": "e667f5bd-c1e9-4303-f8ad-85b8326bfa88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 25  26]\n",
            " [  0 639]]\n"
          ]
        }
      ],
      "source": [
        "# Generate confusion matrix\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQCNDQb1rP6Z",
        "outputId": "34c4558b-b083-4789-f89d-cdac1451c630"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHealthy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArthritis\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(cm, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m, xticklabels\u001b[38;5;241m=\u001b[39mlabels, yticklabels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfusion Matrix\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted Label\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Label\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display the confusion matrix\n",
        "\n",
        "\n",
        "labels = ['Healthy', 'Arthritis']\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
        "\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyAuVhDyTP8t"
      },
      "source": [
        "---\n",
        "**REFERENCES**\n",
        "\n",
        "---\n",
        "https://towardsdatascience.com/an-informative-colab-guide-to-load-image-datasets-from-github-kaggle-and-local-machine-75cae89ffa1e\n",
        "\n",
        "https://pyimagesearch.com/2020/03/16/detecting-covid-19-in-x-ray-images-with-keras-tensorflow-and-deep-learning/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}